{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1 \n",
    "EPSILON_DECAY = 0.998\n",
    "MIN_EPSILON = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNetwork:\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        self.step_size=network_config.get('step_size')\n",
    "    def create_model(self):\n",
    "        i = Input(shape=self.state_dim)\n",
    "        x = Dense(256, activation='relu')(i)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dense(self.num_actions, activation='linear')(x)\n",
    "        model = Model(i, x)\n",
    "        model.compile(optimizer=Adam(lr=self.step_size),loss='mse')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "       \n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "       \n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, agent_config):\n",
    "       \n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        \n",
    "        self.model=self.network.create_model()\n",
    "        \n",
    "        self.target_model=self.network.create_model()\n",
    "        \n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        \n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.epsilon = epsilon\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    \n",
    "    def policy(self, state):\n",
    "        \n",
    "        action_values =self.model.predict(state)\n",
    "        if (np.random.uniform() < self.epsilon) or (action_values.all() == 0):\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "        else:\n",
    "            action=np.argmax(action_values)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self):\n",
    "       \n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = env.reset()\n",
    "        self.last_state = np.reshape(self.last_state,(-1,self.last_state.shape[0]))\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "\n",
    "    def agent_step(self, state,reward,terminal):\n",
    "      \n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        state = np.array([state])\n",
    "       \n",
    "     \n",
    "        action = self.policy(state)\n",
    "       \n",
    "        \n",
    "     \n",
    "\n",
    "       \n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "        \n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                self.agent_train(experiences)\n",
    "        \n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "      \n",
    "        return action\n",
    "\n",
    "    \n",
    "    def agent_train(self,experiences):\n",
    "        states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "        states = np.concatenate(states)\n",
    "        next_states = np.concatenate(next_states)\n",
    "        rewards = np.array(rewards)\n",
    "        terminals = np.array(terminals)\n",
    "        batch_size1 = states.shape[0]\n",
    "        q_next_mat = self.target_model.predict(next_states)\n",
    "        \n",
    "        v_next_vec = np.max(q_next_mat, axis=1)*(1-terminals)\n",
    "        \n",
    "        target_vec = rewards + self.discount*v_next_vec\n",
    "       \n",
    "        q_mat = self.model.predict(states)\n",
    "      \n",
    "        batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "        X=states\n",
    "        q_mat[batch_indices,actions] = target_vec\n",
    " \n",
    "        self.model.fit(X,q_mat,batch_size=batch_size1,verbose=0,shuffle=False)\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {\n",
    "             'network_config': {\n",
    "                 'state_dim': 8,\n",
    "                 'num_actions': 4,\n",
    "                 'step_size':1e-3\n",
    "             },\n",
    "             'replay_buffer_size': 50000,\n",
    "             'minibatch_sz': 64,\n",
    "             'num_replay_updates_per_step': 4,\n",
    "             'gamma': 0.90,\n",
    "             'seed': 0}\n",
    "EPISODES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent(agent_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episode=[]\n",
    "no_episodes=[]\n",
    "episode_steps=[]\n",
    "eps_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score:  -81.04317158377712 epsilon 0.86 steps 73\n",
      "episode:  1 score:  -119.37627152409348 epsilon 0.72 steps 88\n",
      "episode:  2 score:  -272.45360775718507 epsilon 0.59 steps 99\n",
      "episode:  3 score:  -34.9486287899219 epsilon 0.47 steps 119\n",
      "episode:  4 score:  -353.07805096559764 epsilon 0.37 steps 111\n",
      "episode:  5 score:  -179.58176618668378 epsilon 0.30 steps 112\n",
      "episode:  6 score:  -121.3694969239962 epsilon 0.25 steps 92\n",
      "episode:  7 score:  -307.29626626012214 epsilon 0.20 steps 120\n",
      "episode:  8 score:  -98.7651312276115 epsilon 0.13 steps 206\n",
      "episode:  9 score:  -264.99455528060696 epsilon 0.08 steps 237\n",
      "episode:  10 score:  -97.02594286112897 epsilon 0.01 steps 1000\n",
      "episode:  11 score:  -131.83021275977427 epsilon 0.01 steps 1000\n",
      "episode:  12 score:  -117.81204189009395 epsilon 0.01 steps 1000\n",
      "episode:  13 score:  -139.95008580996867 epsilon 0.01 steps 1000\n",
      "episode:  14 score:  -112.44464426925596 epsilon 0.01 steps 1000\n",
      "episode:  15 score:  -137.94856044540978 epsilon 0.01 steps 1000\n",
      "episode:  16 score:  -143.8094254889894 epsilon 0.01 steps 1000\n",
      "episode:  17 score:  -122.72198164491427 epsilon 0.01 steps 1000\n",
      "episode:  18 score:  -262.57281862783344 epsilon 0.01 steps 323\n",
      "episode:  19 score:  -256.36378421118604 epsilon 0.01 steps 205\n",
      "episode:  20 score:  -25.213125649285885 epsilon 0.01 steps 152\n",
      "episode:  21 score:  -122.59823765084411 epsilon 0.01 steps 1000\n",
      "episode:  22 score:  -37.27142176746992 epsilon 0.01 steps 781\n",
      "episode:  23 score:  -106.92634974213317 epsilon 0.01 steps 1000\n",
      "episode:  24 score:  48.64192230889839 epsilon 0.01 steps 168\n",
      "episode:  25 score:  16.237339948376224 epsilon 0.01 steps 1000\n",
      "episode:  26 score:  -67.24974233422499 epsilon 0.01 steps 1000\n",
      "episode:  27 score:  -69.03572616737488 epsilon 0.01 steps 1000\n",
      "episode:  28 score:  -92.15387287687818 epsilon 0.01 steps 1000\n",
      "episode:  29 score:  -114.23568518532913 epsilon 0.01 steps 1000\n",
      "episode:  30 score:  -111.70538915564589 epsilon 0.01 steps 82\n",
      "episode:  31 score:  19.50507829426803 epsilon 0.01 steps 326\n",
      "episode:  32 score:  -104.86918272058037 epsilon 0.01 steps 1000\n",
      "episode:  33 score:  -158.48049826221026 epsilon 0.01 steps 207\n",
      "episode:  34 score:  -74.85339965390389 epsilon 0.01 steps 1000\n",
      "episode:  35 score:  -91.34706506237627 epsilon 0.01 steps 1000\n",
      "episode:  36 score:  -109.54551140601826 epsilon 0.01 steps 1000\n",
      "episode:  37 score:  -25.240273542130076 epsilon 0.01 steps 1000\n",
      "episode:  38 score:  -97.30286707331226 epsilon 0.01 steps 1000\n",
      "episode:  39 score:  -120.38578906137575 epsilon 0.01 steps 1000\n",
      "episode:  40 score:  81.33988161425472 epsilon 0.01 steps 1000\n",
      "episode:  41 score:  -71.3088419355629 epsilon 0.01 steps 1000\n",
      "episode:  42 score:  -106.6273662305252 epsilon 0.01 steps 1000\n",
      "episode:  43 score:  -147.00638913492497 epsilon 0.01 steps 1000\n",
      "episode:  44 score:  -147.52911404504883 epsilon 0.01 steps 1000\n",
      "episode:  45 score:  -94.67684228601279 epsilon 0.01 steps 1000\n",
      "episode:  46 score:  -88.23638248782642 epsilon 0.01 steps 1000\n",
      "episode:  47 score:  -47.86198049679041 epsilon 0.01 steps 1000\n",
      "episode:  48 score:  -134.96155416785564 epsilon 0.01 steps 1000\n",
      "episode:  49 score:  -5.328805799184806 epsilon 0.01 steps 125\n",
      "episode:  50 score:  -113.40335068517068 epsilon 0.01 steps 1000\n",
      "episode:  51 score:  261.3105363588653 epsilon 0.01 steps 267\n",
      "episode:  52 score:  -110.35114570223577 epsilon 0.01 steps 302\n",
      "episode:  53 score:  -147.0572410620098 epsilon 0.01 steps 497\n",
      "episode:  54 score:  -283.0694649426645 epsilon 0.01 steps 585\n",
      "episode:  55 score:  -97.0823956828851 epsilon 0.01 steps 1000\n",
      "episode:  56 score:  -109.99278146532524 epsilon 0.01 steps 1000\n",
      "episode:  57 score:  -317.6158094075993 epsilon 0.01 steps 113\n",
      "episode:  58 score:  -68.52889222327654 epsilon 0.01 steps 1000\n",
      "episode:  59 score:  -368.4952041117395 epsilon 0.01 steps 217\n",
      "episode:  60 score:  -108.65210772192837 epsilon 0.01 steps 1000\n",
      "episode:  61 score:  -146.6063588761118 epsilon 0.01 steps 1000\n",
      "episode:  62 score:  -131.7527024962411 epsilon 0.01 steps 1000\n",
      "episode:  63 score:  -105.25645362323809 epsilon 0.01 steps 1000\n",
      "episode:  64 score:  -23.960363060237157 epsilon 0.01 steps 191\n",
      "episode:  65 score:  -148.42986831454255 epsilon 0.01 steps 1000\n",
      "episode:  66 score:  -136.3021105732431 epsilon 0.01 steps 1000\n",
      "episode:  67 score:  -219.34996015302227 epsilon 0.01 steps 354\n",
      "episode:  68 score:  -37.221060147620285 epsilon 0.01 steps 1000\n",
      "episode:  69 score:  -150.26636562015827 epsilon 0.01 steps 1000\n",
      "episode:  70 score:  -148.21805931168444 epsilon 0.01 steps 1000\n",
      "episode:  71 score:  -125.58359483989881 epsilon 0.01 steps 1000\n",
      "episode:  72 score:  -88.78471664480936 epsilon 0.01 steps 1000\n",
      "episode:  73 score:  -81.95541156415749 epsilon 0.01 steps 281\n",
      "episode:  74 score:  -133.73716797733735 epsilon 0.01 steps 1000\n",
      "episode:  75 score:  -133.15284034233372 epsilon 0.01 steps 355\n",
      "episode:  76 score:  -115.73797112371395 epsilon 0.01 steps 1000\n",
      "episode:  77 score:  -155.09627875605287 epsilon 0.01 steps 1000\n",
      "episode:  78 score:  -87.21451546128286 epsilon 0.01 steps 1000\n",
      "episode:  79 score:  -91.52201414817314 epsilon 0.01 steps 1000\n",
      "episode:  80 score:  -110.17994648870433 epsilon 0.01 steps 1000\n",
      "episode:  81 score:  -154.06936098010505 epsilon 0.01 steps 1000\n",
      "episode:  82 score:  -92.46292904745165 epsilon 0.01 steps 1000\n",
      "episode:  83 score:  -156.90441999421282 epsilon 0.01 steps 1000\n",
      "episode:  84 score:  -133.3415403136252 epsilon 0.01 steps 1000\n",
      "episode:  85 score:  -105.73159400416723 epsilon 0.01 steps 1000\n",
      "episode:  86 score:  -82.96169490762576 epsilon 0.01 steps 1000\n",
      "episode:  87 score:  -81.1372082149718 epsilon 0.01 steps 1000\n",
      "episode:  88 score:  -54.972292868034025 epsilon 0.01 steps 1000\n",
      "episode:  89 score:  -100.20753154024186 epsilon 0.01 steps 1000\n",
      "episode:  90 score:  -105.52516362115671 epsilon 0.01 steps 1000\n",
      "episode:  91 score:  -109.80541996119454 epsilon 0.01 steps 1000\n",
      "episode:  92 score:  -112.15021268450171 epsilon 0.01 steps 1000\n",
      "episode:  93 score:  -142.24869967812685 epsilon 0.01 steps 1000\n",
      "episode:  94 score:  -110.9268357948439 epsilon 0.01 steps 1000\n",
      "episode:  95 score:  -112.43249102673407 epsilon 0.01 steps 1000\n",
      "episode:  96 score:  -105.19833646016656 epsilon 0.01 steps 1000\n",
      "episode:  97 score:  -106.45481431976707 epsilon 0.01 steps 1000\n",
      "episode:  98 score:  -120.28540941704878 epsilon 0.01 steps 1000\n",
      "episode:  99 score:  -135.46110833180353 epsilon 0.01 steps 1000\n",
      "episode:  100 score:  -124.37590756434624 epsilon 0.01 steps 1000\n",
      "episode:  101 score:  -119.19124694071624 epsilon 0.01 steps 1000\n",
      "episode:  102 score:  -82.78685737306631 epsilon 0.01 steps 1000\n",
      "episode:  103 score:  -124.7196614063428 epsilon 0.01 steps 1000\n",
      "episode:  104 score:  -84.38521018475082 epsilon 0.01 steps 1000\n",
      "episode:  105 score:  -140.5307179482909 epsilon 0.01 steps 1000\n",
      "episode:  106 score:  -140.85223975996132 epsilon 0.01 steps 1000\n",
      "episode:  107 score:  -96.94082530723219 epsilon 0.01 steps 1000\n",
      "episode:  108 score:  -88.77031535140114 epsilon 0.01 steps 1000\n",
      "episode:  109 score:  -115.8283894659612 epsilon 0.01 steps 1000\n",
      "episode:  110 score:  -152.9615906677231 epsilon 0.01 steps 956\n",
      "episode:  111 score:  -162.62037895632486 epsilon 0.01 steps 1000\n",
      "episode:  112 score:  -131.83179586421062 epsilon 0.01 steps 1000\n",
      "episode:  113 score:  -153.56028776835407 epsilon 0.01 steps 1000\n",
      "episode:  114 score:  -104.99068257668021 epsilon 0.01 steps 1000\n",
      "episode:  115 score:  -143.14803431705337 epsilon 0.01 steps 1000\n",
      "episode:  116 score:  -119.80968142783288 epsilon 0.01 steps 1000\n",
      "episode:  117 score:  -96.79505945035889 epsilon 0.01 steps 1000\n",
      "episode:  118 score:  -126.76339338546224 epsilon 0.01 steps 1000\n",
      "episode:  119 score:  -64.29128453110592 epsilon 0.01 steps 1000\n",
      "episode:  120 score:  -70.43586248660237 epsilon 0.01 steps 1000\n",
      "episode:  121 score:  -137.90118624394688 epsilon 0.01 steps 1000\n",
      "episode:  122 score:  -118.56845149383368 epsilon 0.01 steps 1000\n",
      "episode:  123 score:  -158.25370659772113 epsilon 0.01 steps 308\n",
      "episode:  124 score:  -97.81995011365144 epsilon 0.01 steps 1000\n",
      "episode:  125 score:  -113.72072111509615 epsilon 0.01 steps 1000\n",
      "episode:  126 score:  -44.774404347341815 epsilon 0.01 steps 1000\n",
      "episode:  127 score:  -111.48722409489255 epsilon 0.01 steps 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  128 score:  -83.32299992098699 epsilon 0.01 steps 1000\n",
      "episode:  129 score:  -124.0438830855853 epsilon 0.01 steps 1000\n",
      "episode:  130 score:  -127.52112580555085 epsilon 0.01 steps 1000\n",
      "episode:  131 score:  -91.7522990301931 epsilon 0.01 steps 1000\n",
      "episode:  132 score:  -106.76904786668416 epsilon 0.01 steps 1000\n",
      "episode:  133 score:  -122.31570683546747 epsilon 0.01 steps 1000\n",
      "episode:  134 score:  -373.4513252067527 epsilon 0.01 steps 993\n",
      "episode:  135 score:  -99.4675892937309 epsilon 0.01 steps 1000\n",
      "episode:  136 score:  -162.1371146277923 epsilon 0.01 steps 1000\n",
      "episode:  137 score:  -134.13277857224315 epsilon 0.01 steps 1000\n",
      "episode:  138 score:  -395.20293591725095 epsilon 0.01 steps 864\n",
      "episode:  139 score:  -426.8945612751724 epsilon 0.01 steps 147\n",
      "episode:  140 score:  -139.3158786597889 epsilon 0.01 steps 1000\n",
      "episode:  141 score:  -102.30278971391184 epsilon 0.01 steps 1000\n",
      "episode:  142 score:  -144.69235742868574 epsilon 0.01 steps 1000\n",
      "episode:  143 score:  -111.97487234604903 epsilon 0.01 steps 198\n",
      "episode:  144 score:  -103.28067304523502 epsilon 0.01 steps 185\n",
      "episode:  145 score:  -88.43050669765603 epsilon 0.01 steps 1000\n",
      "episode:  146 score:  -63.06987894472263 epsilon 0.01 steps 90\n",
      "episode:  147 score:  -106.03322603071038 epsilon 0.01 steps 1000\n",
      "episode:  148 score:  -84.52353174491671 epsilon 0.01 steps 1000\n",
      "episode:  149 score:  -119.13129047824161 epsilon 0.01 steps 1000\n",
      "episode:  150 score:  -119.51120134046775 epsilon 0.01 steps 1000\n",
      "episode:  151 score:  -66.20278629154386 epsilon 0.01 steps 1000\n",
      "episode:  152 score:  -98.1273736708188 epsilon 0.01 steps 1000\n",
      "episode:  153 score:  -52.13546054362614 epsilon 0.01 steps 1000\n",
      "episode:  154 score:  -134.00303646193956 epsilon 0.01 steps 1000\n",
      "episode:  155 score:  -100.59402337826545 epsilon 0.01 steps 1000\n",
      "episode:  156 score:  -88.94767018814855 epsilon 0.01 steps 1000\n",
      "episode:  157 score:  -116.68909472385643 epsilon 0.01 steps 1000\n",
      "episode:  158 score:  -85.82269892420264 epsilon 0.01 steps 1000\n",
      "episode:  159 score:  -122.46415379955727 epsilon 0.01 steps 1000\n",
      "episode:  160 score:  -90.24336586335924 epsilon 0.01 steps 1000\n",
      "episode:  161 score:  -68.84292864391428 epsilon 0.01 steps 1000\n",
      "episode:  162 score:  -101.1518747293514 epsilon 0.01 steps 1000\n",
      "episode:  163 score:  -120.95877649748522 epsilon 0.01 steps 1000\n",
      "episode:  164 score:  -87.06652657852901 epsilon 0.01 steps 1000\n",
      "episode:  165 score:  -96.67613637116048 epsilon 0.01 steps 1000\n",
      "episode:  166 score:  -143.86861373042092 epsilon 0.01 steps 1000\n",
      "episode:  167 score:  -86.61475510510368 epsilon 0.01 steps 1000\n",
      "episode:  168 score:  -143.13064885060365 epsilon 0.01 steps 1000\n",
      "episode:  169 score:  -130.71937896893613 epsilon 0.01 steps 1000\n",
      "episode:  170 score:  -128.60027790008462 epsilon 0.01 steps 1000\n",
      "episode:  171 score:  -77.69268141847414 epsilon 0.01 steps 1000\n",
      "episode:  172 score:  -56.245880648253845 epsilon 0.01 steps 1000\n",
      "episode:  173 score:  -113.19565675837086 epsilon 0.01 steps 1000\n",
      "episode:  174 score:  -231.48735681786553 epsilon 0.01 steps 100\n",
      "episode:  175 score:  -112.53325122121996 epsilon 0.01 steps 1000\n",
      "episode:  176 score:  -128.5042439178045 epsilon 0.01 steps 1000\n",
      "episode:  177 score:  -123.22215866011176 epsilon 0.01 steps 1000\n",
      "episode:  178 score:  -85.031022507608 epsilon 0.01 steps 329\n",
      "episode:  179 score:  -81.46661236028244 epsilon 0.01 steps 1000\n",
      "episode:  180 score:  -169.0429950384066 epsilon 0.01 steps 201\n",
      "episode:  181 score:  -47.66989520455671 epsilon 0.01 steps 1000\n",
      "episode:  182 score:  -84.650463330305 epsilon 0.01 steps 283\n",
      "episode:  183 score:  -132.47356111114175 epsilon 0.01 steps 1000\n",
      "episode:  184 score:  -91.15484146422004 epsilon 0.01 steps 1000\n",
      "episode:  185 score:  -83.61484212655596 epsilon 0.01 steps 1000\n",
      "episode:  186 score:  -60.886762818917845 epsilon 0.01 steps 1000\n",
      "episode:  187 score:  -93.11652964637778 epsilon 0.01 steps 1000\n",
      "episode:  188 score:  -83.33864689810534 epsilon 0.01 steps 1000\n",
      "episode:  189 score:  -99.99289235075972 epsilon 0.01 steps 1000\n",
      "episode:  190 score:  -101.56510255922515 epsilon 0.01 steps 148\n",
      "episode:  191 score:  -103.59487250296328 epsilon 0.01 steps 1000\n",
      "episode:  192 score:  -80.04018274697252 epsilon 0.01 steps 1000\n",
      "episode:  193 score:  -97.05260474403057 epsilon 0.01 steps 1000\n",
      "episode:  194 score:  -108.46743326224002 epsilon 0.01 steps 1000\n",
      "episode:  195 score:  -154.8007475143278 epsilon 0.01 steps 411\n",
      "episode:  196 score:  -126.12975176272649 epsilon 0.01 steps 1000\n",
      "episode:  197 score:  -141.70407397634636 epsilon 0.01 steps 1000\n",
      "episode:  198 score:  -77.4578672124231 epsilon 0.01 steps 1000\n",
      "episode:  199 score:  -180.57524885762365 epsilon 0.01 steps 376\n",
      "episode:  200 score:  -81.85687347004168 epsilon 0.01 steps 1000\n",
      "episode:  201 score:  -146.67421888374056 epsilon 0.01 steps 1000\n",
      "episode:  202 score:  -124.76145585415736 epsilon 0.01 steps 1000\n",
      "episode:  203 score:  -110.88170043935881 epsilon 0.01 steps 1000\n",
      "episode:  204 score:  -125.64206037311443 epsilon 0.01 steps 1000\n",
      "episode:  205 score:  -914.3767775249189 epsilon 0.01 steps 121\n",
      "episode:  206 score:  -90.8228515131146 epsilon 0.01 steps 1000\n",
      "episode:  207 score:  -115.03174562867052 epsilon 0.01 steps 1000\n",
      "episode:  208 score:  -109.58385608593184 epsilon 0.01 steps 228\n",
      "episode:  209 score:  -154.3805814626598 epsilon 0.01 steps 1000\n",
      "episode:  210 score:  -250.10075774845768 epsilon 0.01 steps 165\n",
      "episode:  211 score:  -86.32851578948923 epsilon 0.01 steps 1000\n",
      "episode:  212 score:  -154.15650800696494 epsilon 0.01 steps 247\n",
      "episode:  213 score:  -259.7733271070234 epsilon 0.01 steps 331\n",
      "episode:  214 score:  -231.38456318014607 epsilon 0.01 steps 664\n",
      "episode:  215 score:  -101.07670940005625 epsilon 0.01 steps 1000\n",
      "episode:  216 score:  -102.4331381250913 epsilon 0.01 steps 1000\n",
      "episode:  217 score:  -77.48758375230886 epsilon 0.01 steps 1000\n",
      "episode:  218 score:  -109.58009684347026 epsilon 0.01 steps 1000\n",
      "episode:  219 score:  -89.22649970692545 epsilon 0.01 steps 1000\n",
      "episode:  220 score:  -83.13157441227645 epsilon 0.01 steps 1000\n",
      "episode:  221 score:  -169.94286913405222 epsilon 0.01 steps 203\n",
      "episode:  222 score:  -102.70069955925143 epsilon 0.01 steps 210\n",
      "episode:  223 score:  -161.3670227001961 epsilon 0.01 steps 1000\n",
      "episode:  224 score:  -97.58575156722021 epsilon 0.01 steps 1000\n",
      "episode:  225 score:  -125.90829300175874 epsilon 0.01 steps 1000\n",
      "episode:  226 score:  -520.3908556299361 epsilon 0.01 steps 180\n",
      "episode:  227 score:  -115.19345213728107 epsilon 0.01 steps 1000\n",
      "episode:  228 score:  -149.45174611633126 epsilon 0.01 steps 1000\n",
      "episode:  229 score:  -133.23223474805977 epsilon 0.01 steps 1000\n",
      "episode:  230 score:  -143.67583146689395 epsilon 0.01 steps 1000\n",
      "episode:  231 score:  -128.11981463432738 epsilon 0.01 steps 1000\n",
      "episode:  232 score:  -122.37166938138114 epsilon 0.01 steps 1000\n",
      "episode:  233 score:  -134.19589905535568 epsilon 0.01 steps 236\n",
      "episode:  234 score:  -123.4220222083315 epsilon 0.01 steps 1000\n",
      "episode:  235 score:  -218.92527170227945 epsilon 0.01 steps 634\n",
      "episode:  236 score:  -72.2124099280024 epsilon 0.01 steps 1000\n",
      "episode:  237 score:  -143.87794313515838 epsilon 0.01 steps 1000\n",
      "episode:  238 score:  -147.26062541130545 epsilon 0.01 steps 460\n",
      "episode:  239 score:  -82.03996521628981 epsilon 0.01 steps 1000\n",
      "episode:  240 score:  -62.88766475615914 epsilon 0.01 steps 1000\n",
      "episode:  241 score:  -114.99779571298615 epsilon 0.01 steps 1000\n",
      "episode:  242 score:  -74.59964176151261 epsilon 0.01 steps 1000\n",
      "episode:  243 score:  -55.426915127598804 epsilon 0.01 steps 1000\n",
      "episode:  244 score:  -140.8129336714062 epsilon 0.01 steps 380\n",
      "episode:  245 score:  -111.07816578642456 epsilon 0.01 steps 1000\n",
      "episode:  246 score:  -83.60575528211126 epsilon 0.01 steps 1000\n",
      "episode:  247 score:  -147.44780436637024 epsilon 0.01 steps 1000\n",
      "episode:  248 score:  -92.77068662706819 epsilon 0.01 steps 1000\n",
      "episode:  249 score:  -123.04461566613179 epsilon 0.01 steps 1000\n",
      "episode:  250 score:  -113.82320174925115 epsilon 0.01 steps 1000\n",
      "episode:  251 score:  -126.4874390753185 epsilon 0.01 steps 1000\n",
      "episode:  252 score:  -78.899534386357 epsilon 0.01 steps 1000\n",
      "episode:  253 score:  -117.15836755086531 epsilon 0.01 steps 398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  254 score:  -131.68584469769996 epsilon 0.01 steps 1000\n",
      "episode:  255 score:  -78.84369929197999 epsilon 0.01 steps 1000\n",
      "episode:  256 score:  -148.4385792778489 epsilon 0.01 steps 1000\n",
      "episode:  257 score:  -101.54206046487782 epsilon 0.01 steps 1000\n",
      "episode:  258 score:  -109.62390103293421 epsilon 0.01 steps 1000\n",
      "episode:  259 score:  -153.48808831659989 epsilon 0.01 steps 217\n",
      "episode:  260 score:  -121.60687178261207 epsilon 0.01 steps 1000\n",
      "episode:  261 score:  -108.73764058096002 epsilon 0.01 steps 1000\n",
      "episode:  262 score:  -108.93711645981818 epsilon 0.01 steps 218\n",
      "episode:  263 score:  -382.59692619038697 epsilon 0.01 steps 436\n",
      "episode:  264 score:  -98.2119029238384 epsilon 0.01 steps 1000\n",
      "episode:  265 score:  -128.83827579051768 epsilon 0.01 steps 1000\n",
      "episode:  266 score:  -141.10082232293757 epsilon 0.01 steps 1000\n",
      "episode:  267 score:  -95.93235187790113 epsilon 0.01 steps 1000\n",
      "episode:  268 score:  -104.87308466059682 epsilon 0.01 steps 190\n",
      "episode:  269 score:  -123.84974130542345 epsilon 0.01 steps 1000\n",
      "episode:  270 score:  -132.2529128837756 epsilon 0.01 steps 1000\n",
      "episode:  271 score:  -110.34104156673607 epsilon 0.01 steps 1000\n",
      "episode:  272 score:  -79.3552385657983 epsilon 0.01 steps 1000\n",
      "episode:  273 score:  -160.47285126353557 epsilon 0.01 steps 985\n",
      "episode:  274 score:  -43.25428233146586 epsilon 0.01 steps 1000\n",
      "episode:  275 score:  -136.58851795926037 epsilon 0.01 steps 291\n",
      "episode:  276 score:  -168.1430994884136 epsilon 0.01 steps 353\n",
      "episode:  277 score:  -272.4741278245102 epsilon 0.01 steps 739\n"
     ]
    }
   ],
   "source": [
    "for episode in range(0,500):\n",
    "    action=agent.agent_start()\n",
    "    terminal=0\n",
    "    while terminal!=1:\n",
    "        state,reward,terminal,info=env.step(action)\n",
    "        if terminal==True:\n",
    "            terminal=1\n",
    "        else:\n",
    "            terminal=0\n",
    "        action=agent.agent_step(state,reward,terminal)\n",
    "        if agent.epsilon > MIN_EPSILON:\n",
    "            agent.epsilon *= EPSILON_DECAY\n",
    "            agent.epsilon = max(MIN_EPSILON,agent.epsilon)\n",
    "    reward = agent.agent_message('get_sum_reward')\n",
    "    reward_episode.append(reward)\n",
    "    no_episodes.append(episode)\n",
    "    episode_steps.append(agent.episode_steps)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    print('episode: ', episode,'score: ',reward,\n",
    "            'epsilon %.2f' % agent.epsilon, 'steps', agent.episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
