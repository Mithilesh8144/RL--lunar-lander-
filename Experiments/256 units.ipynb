{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores, epsilons, filename, lines=None):\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111, label=\"1\")\n",
    "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "    ax.plot(x, epsilons, color=\"C0\")\n",
    "    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "    ax.tick_params(axis='x', colors=\"C0\")\n",
    "    ax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "    ax2.scatter(x, running_avg, color=\"C1\")\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel('Score', color=\"C1\")\n",
    "    ax2.yaxis.set_label_position('right')\n",
    "    ax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            plt.axvline(x=line)\n",
    "\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1  \n",
    "EPSILON_DECAY = 0.995 \n",
    "MIN_EPSILON = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNetwork:\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        self.step_size=network_config.get('step_size')\n",
    "    def create_model(self):\n",
    "        i = Input(shape=self.state_dim)\n",
    "        x = Dense(256, activation='relu')(i)\n",
    "        x = Dense(self.num_actions, activation='linear')(x)\n",
    "        model = Model(i, x)\n",
    "        model.compile(optimizer=Adam(lr=self.step_size),loss='mse')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_config= {\n",
    "                 'state_dim': 8,\n",
    "                 'num_actions': 4,\n",
    "                 'step_size':1e-3\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=ActionValueNetwork(network_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=k.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 3,332\n",
      "Trainable params: 3,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "\n",
    "      \n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "     \n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "      \n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_config):\n",
    "    \n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        \n",
    "        self.model=self.network.create_model()\n",
    "        \n",
    "        self.target_model=self.network.create_model()\n",
    "        \n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        \n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.epsilon = epsilon\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    \n",
    "    def policy(self, state):\n",
    "      \n",
    "        action_values =self.model.predict(state)\n",
    "        if (np.random.uniform() < self.epsilon):\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "        else:\n",
    "            action=np.argmax(action_values)\n",
    "        return action\n",
    "\n",
    "   \n",
    "    def agent_start(self):\n",
    "       \n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = env.reset()\n",
    "        self.last_state = np.reshape(self.last_state,(-1,self.last_state.shape[0]))\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "\n",
    "    def agent_step(self, state,reward,terminal):\n",
    "      \n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        \n",
    "        state = np.array([state])\n",
    "      \n",
    "     \n",
    "        action = self.policy(state)\n",
    "       \n",
    "        \n",
    "\n",
    "       \n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, terminal, state)\n",
    "       \n",
    "        \n",
    "        \n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                self.agent_train(experiences)\n",
    "       \n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "      \n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "    def agent_train(self,experiences):\n",
    "        states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "        states = np.concatenate(states)\n",
    "        next_states = np.concatenate(next_states)\n",
    "        rewards = np.array(rewards)\n",
    "        terminals = np.array(terminals)\n",
    "        batch_size1 = states.shape[0]\n",
    "        \n",
    "        q_next_mat = self.target_model.predict(next_states)\n",
    "        \n",
    "        v_next_vec = np.max(q_next_mat, axis=1)*(1-terminals)\n",
    "        \n",
    "        target_vec = rewards + self.discount*v_next_vec\n",
    "        \n",
    "        q_mat = self.model.predict(states)\n",
    "        \n",
    "        batch_indices = np.arange(q_mat.shape[0])\n",
    "        \n",
    "        q_mat[batch_indices,actions] = target_vec\n",
    "        X=states\n",
    "        q_mat[batch_indices,actions] = target_vec\n",
    "        self.model.fit(X,q_mat,batch_size=batch_size1,verbose=0,shuffle=False)\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {\n",
    "             'network_config': {\n",
    "                 'state_dim': 8,\n",
    "                 'num_actions': 4,\n",
    "                 'step_size':1e-3\n",
    "             },\n",
    "             'replay_buffer_size': 50000,\n",
    "             'minibatch_sz': 64,\n",
    "             'num_replay_updates_per_step': 4,\n",
    "             'gamma': 0.99,\n",
    "             'seed': 0}\n",
    "EPISODES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent(agent_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episode=[]\n",
    "no_episodes=[]\n",
    "episode_steps=[]\n",
    "epsilon_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score:  -352.6464069255609 epsilon 0.61 steps 98\n",
      "episode:  1 score:  -360.8051091745731 epsilon 0.31 steps 134\n",
      "episode:  2 score:  -221.198762818357 epsilon 0.17 steps 118\n",
      "episode:  3 score:  -96.31208567747527 epsilon 0.07 steps 193\n",
      "episode:  4 score:  -152.89634745560977 epsilon 0.03 steps 179\n",
      "episode:  5 score:  -116.91340002696666 epsilon 0.01 steps 321\n",
      "episode:  6 score:  -12.52097852451098 epsilon 0.01 steps 253\n",
      "episode:  7 score:  -75.72012842864669 epsilon 0.01 steps 216\n",
      "episode:  8 score:  -37.85200864642361 epsilon 0.01 steps 226\n",
      "episode:  9 score:  28.46728851428915 epsilon 0.01 steps 249\n",
      "episode:  10 score:  47.99370847988746 epsilon 0.01 steps 306\n",
      "episode:  11 score:  -224.42196342842612 epsilon 0.01 steps 460\n",
      "episode:  12 score:  -88.66180742943585 epsilon 0.01 steps 209\n",
      "episode:  13 score:  -132.71609711483387 epsilon 0.01 steps 643\n",
      "episode:  14 score:  -120.59914714847973 epsilon 0.01 steps 1000\n",
      "episode:  15 score:  -96.63980214487773 epsilon 0.01 steps 607\n",
      "episode:  16 score:  -75.32538992514536 epsilon 0.01 steps 508\n",
      "episode:  17 score:  -8.651633846800323 epsilon 0.01 steps 409\n",
      "episode:  18 score:  -43.70729311197451 epsilon 0.01 steps 299\n",
      "episode:  19 score:  164.16326635208736 epsilon 0.01 steps 788\n",
      "episode:  20 score:  -89.2735477511093 epsilon 0.01 steps 395\n",
      "episode:  21 score:  182.60992476331506 epsilon 0.01 steps 757\n",
      "episode:  22 score:  25.3551073950516 epsilon 0.01 steps 1000\n",
      "episode:  23 score:  98.117257752044 epsilon 0.01 steps 812\n",
      "episode:  24 score:  -67.35368628822144 epsilon 0.01 steps 186\n",
      "episode:  25 score:  -119.40985495910161 epsilon 0.01 steps 92\n",
      "episode:  26 score:  2.322372480820377 epsilon 0.01 steps 170\n",
      "episode:  27 score:  170.90998151576275 epsilon 0.01 steps 747\n",
      "episode:  28 score:  -10.418230247096176 epsilon 0.01 steps 198\n",
      "episode:  29 score:  216.35188526869644 epsilon 0.01 steps 465\n",
      "episode:  30 score:  -161.54542416167953 epsilon 0.01 steps 133\n",
      "episode:  31 score:  -188.10354830241386 epsilon 0.01 steps 158\n",
      "episode:  32 score:  -35.69265019108278 epsilon 0.01 steps 265\n",
      "episode:  33 score:  -287.9616159634348 epsilon 0.01 steps 633\n",
      "episode:  34 score:  179.15611509871803 epsilon 0.01 steps 807\n",
      "episode:  35 score:  170.8050476116495 epsilon 0.01 steps 801\n",
      "episode:  36 score:  163.87939179550614 epsilon 0.01 steps 515\n",
      "episode:  37 score:  130.04348162080814 epsilon 0.01 steps 982\n",
      "episode:  38 score:  103.6764256119324 epsilon 0.01 steps 1000\n",
      "episode:  39 score:  157.98789984819254 epsilon 0.01 steps 620\n",
      "episode:  40 score:  26.27819091198008 epsilon 0.01 steps 1000\n",
      "episode:  41 score:  59.17967987663007 epsilon 0.01 steps 1000\n",
      "episode:  42 score:  126.8343214139184 epsilon 0.01 steps 961\n",
      "episode:  43 score:  211.50411059002258 epsilon 0.01 steps 556\n",
      "episode:  44 score:  44.73971203530939 epsilon 0.01 steps 1000\n",
      "episode:  45 score:  182.06973127224765 epsilon 0.01 steps 462\n",
      "episode:  46 score:  -133.28936048762188 epsilon 0.01 steps 135\n",
      "episode:  47 score:  219.77675791971768 epsilon 0.01 steps 824\n",
      "episode:  48 score:  -192.44185565914674 epsilon 0.01 steps 547\n",
      "episode:  49 score:  75.65297164811588 epsilon 0.01 steps 1000\n",
      "episode:  50 score:  -89.74686535636692 epsilon 0.01 steps 78\n",
      "episode:  51 score:  93.82944183191948 epsilon 0.01 steps 1000\n",
      "episode:  52 score:  -267.20131513134424 epsilon 0.01 steps 197\n",
      "episode:  53 score:  54.10015538699425 epsilon 0.01 steps 1000\n",
      "episode:  54 score:  -37.46663667589212 epsilon 0.01 steps 368\n",
      "episode:  55 score:  -269.99078885961814 epsilon 0.01 steps 643\n",
      "episode:  56 score:  212.95651109940937 epsilon 0.01 steps 854\n",
      "episode:  57 score:  -41.469800402911275 epsilon 0.01 steps 237\n",
      "episode:  58 score:  233.52635872935804 epsilon 0.01 steps 634\n",
      "episode:  59 score:  31.650554233776855 epsilon 0.01 steps 1000\n",
      "episode:  60 score:  115.35847037989979 epsilon 0.01 steps 898\n",
      "episode:  61 score:  -32.50708269687249 epsilon 0.01 steps 275\n",
      "episode:  62 score:  -166.87219287773078 epsilon 0.01 steps 530\n",
      "episode:  63 score:  239.18400687114345 epsilon 0.01 steps 520\n",
      "episode:  64 score:  10.813572088642925 epsilon 0.01 steps 1000\n",
      "episode:  65 score:  202.45645321414906 epsilon 0.01 steps 636\n",
      "episode:  66 score:  209.98191316979546 epsilon 0.01 steps 986\n",
      "episode:  67 score:  -155.8249801233692 epsilon 0.01 steps 73\n",
      "episode:  68 score:  186.5826055006351 epsilon 0.01 steps 811\n",
      "episode:  69 score:  224.6068270269852 epsilon 0.01 steps 309\n",
      "episode:  70 score:  131.44872349269417 epsilon 0.01 steps 620\n",
      "episode:  71 score:  -46.95919693680737 epsilon 0.01 steps 245\n",
      "episode:  72 score:  174.40451024023236 epsilon 0.01 steps 832\n",
      "episode:  73 score:  222.55988511981747 epsilon 0.01 steps 636\n",
      "episode:  74 score:  227.38804899868316 epsilon 0.01 steps 678\n",
      "episode:  75 score:  106.42638175980518 epsilon 0.01 steps 526\n",
      "episode:  76 score:  262.6633590778813 epsilon 0.01 steps 684\n",
      "episode:  77 score:  148.39561672696163 epsilon 0.01 steps 589\n",
      "episode:  78 score:  229.52639730623258 epsilon 0.01 steps 556\n",
      "episode:  79 score:  203.96217850572057 epsilon 0.01 steps 709\n",
      "episode:  80 score:  -166.04283441266597 epsilon 0.01 steps 263\n",
      "episode:  81 score:  241.96295722764197 epsilon 0.01 steps 442\n",
      "episode:  82 score:  238.2469837823392 epsilon 0.01 steps 502\n",
      "episode:  83 score:  -201.763700193634 epsilon 0.01 steps 376\n",
      "episode:  84 score:  -162.15291238356826 epsilon 0.01 steps 267\n",
      "episode:  85 score:  -15.507878601621087 epsilon 0.01 steps 388\n",
      "episode:  86 score:  97.16919826153183 epsilon 0.01 steps 1000\n",
      "episode:  87 score:  -85.97531704227191 epsilon 0.01 steps 290\n",
      "episode:  88 score:  229.47569995086292 epsilon 0.01 steps 919\n",
      "episode:  89 score:  234.9554387213561 epsilon 0.01 steps 561\n",
      "episode:  90 score:  -58.07676347418775 epsilon 0.01 steps 469\n",
      "episode:  91 score:  -8.0073350354768 epsilon 0.01 steps 189\n",
      "episode:  92 score:  247.5812399184657 epsilon 0.01 steps 419\n",
      "episode:  93 score:  191.81086554437258 epsilon 0.01 steps 406\n",
      "episode:  94 score:  -151.91027490149804 epsilon 0.01 steps 163\n",
      "episode:  95 score:  -24.458777905422465 epsilon 0.01 steps 242\n",
      "episode:  96 score:  -113.49874071609234 epsilon 0.01 steps 217\n",
      "episode:  97 score:  241.87737364427724 epsilon 0.01 steps 585\n",
      "episode:  98 score:  250.46087288473666 epsilon 0.01 steps 378\n",
      "episode:  99 score:  182.35673893753835 epsilon 0.01 steps 625\n",
      "episode:  100 score:  211.34242604656987 epsilon 0.01 steps 467\n",
      "episode:  101 score:  200.4783550805251 epsilon 0.01 steps 419\n",
      "episode:  102 score:  211.3981289377134 epsilon 0.01 steps 452\n",
      "episode:  103 score:  216.59457718845908 epsilon 0.01 steps 999\n",
      "episode:  104 score:  -218.79723341435195 epsilon 0.01 steps 259\n",
      "episode:  105 score:  37.1911491861879 epsilon 0.01 steps 1000\n",
      "episode:  106 score:  198.8633171922255 epsilon 0.01 steps 393\n",
      "episode:  107 score:  257.83006544223906 epsilon 0.01 steps 488\n",
      "episode:  108 score:  -147.58844715443988 epsilon 0.01 steps 201\n",
      "episode:  109 score:  220.71734131035225 epsilon 0.01 steps 763\n",
      "episode:  110 score:  -196.7542615004637 epsilon 0.01 steps 279\n",
      "episode:  111 score:  -77.91828721902435 epsilon 0.01 steps 136\n",
      "episode:  112 score:  257.70645440928536 epsilon 0.01 steps 444\n",
      "episode:  113 score:  -85.22512862902258 epsilon 0.01 steps 142\n",
      "episode:  114 score:  70.08638854911068 epsilon 0.01 steps 1000\n",
      "episode:  115 score:  -185.5533036981318 epsilon 0.01 steps 817\n",
      "episode:  116 score:  194.0161549490209 epsilon 0.01 steps 474\n",
      "episode:  117 score:  -147.28404134461775 epsilon 0.01 steps 83\n",
      "episode:  118 score:  176.15854759755916 epsilon 0.01 steps 588\n",
      "episode:  119 score:  213.05332071431454 epsilon 0.01 steps 760\n",
      "episode:  120 score:  242.4439335064504 epsilon 0.01 steps 406\n",
      "episode:  121 score:  -112.61405680628295 epsilon 0.01 steps 196\n",
      "episode:  122 score:  204.5747812060883 epsilon 0.01 steps 502\n",
      "episode:  123 score:  -93.9722938556451 epsilon 0.01 steps 190\n",
      "episode:  124 score:  196.72678915593423 epsilon 0.01 steps 608\n",
      "episode:  125 score:  209.22140538130776 epsilon 0.01 steps 470\n",
      "episode:  126 score:  227.6153805663675 epsilon 0.01 steps 520\n",
      "episode:  127 score:  -101.84991127299222 epsilon 0.01 steps 150\n",
      "episode:  128 score:  277.1056216807979 epsilon 0.01 steps 668\n",
      "episode:  129 score:  94.17862060688155 epsilon 0.01 steps 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  130 score:  209.6475815750456 epsilon 0.01 steps 463\n",
      "episode:  131 score:  -94.49704826640534 epsilon 0.01 steps 383\n",
      "episode:  132 score:  258.67753069591913 epsilon 0.01 steps 380\n",
      "episode:  133 score:  96.08160941178538 epsilon 0.01 steps 1000\n",
      "episode:  134 score:  214.73535975089388 epsilon 0.01 steps 494\n",
      "episode:  135 score:  148.87878942696327 epsilon 0.01 steps 1000\n",
      "episode:  136 score:  225.07962649671276 epsilon 0.01 steps 595\n",
      "episode:  137 score:  123.72584578496411 epsilon 0.01 steps 1000\n",
      "episode:  138 score:  270.41581945853545 epsilon 0.01 steps 453\n",
      "episode:  139 score:  263.649826002744 epsilon 0.01 steps 528\n",
      "episode:  140 score:  141.3523723384864 epsilon 0.01 steps 1000\n",
      "episode:  141 score:  126.44168514602333 epsilon 0.01 steps 1000\n",
      "episode:  142 score:  -161.5644805765101 epsilon 0.01 steps 162\n",
      "episode:  143 score:  -18.940759812855163 epsilon 0.01 steps 248\n",
      "episode:  144 score:  -223.48085773390235 epsilon 0.01 steps 192\n",
      "episode:  145 score:  263.2658261073609 epsilon 0.01 steps 409\n",
      "episode:  146 score:  -109.65929958252453 epsilon 0.01 steps 166\n",
      "episode:  147 score:  -136.54179088338495 epsilon 0.01 steps 122\n",
      "episode:  148 score:  95.10021820278625 epsilon 0.01 steps 1000\n",
      "episode:  149 score:  -25.094896002098565 epsilon 0.01 steps 311\n",
      "episode:  150 score:  -263.7293701486732 epsilon 0.01 steps 344\n",
      "episode:  151 score:  123.31863093300855 epsilon 0.01 steps 1000\n",
      "episode:  152 score:  130.15832959354418 epsilon 0.01 steps 1000\n",
      "episode:  153 score:  116.20513097318519 epsilon 0.01 steps 1000\n",
      "episode:  154 score:  111.33650915119604 epsilon 0.01 steps 1000\n",
      "episode:  155 score:  90.70457868443371 epsilon 0.01 steps 1000\n",
      "episode:  156 score:  105.20635126723668 epsilon 0.01 steps 1000\n",
      "episode:  157 score:  98.49717367099282 epsilon 0.01 steps 1000\n",
      "episode:  158 score:  273.2890650363433 epsilon 0.01 steps 489\n",
      "episode:  159 score:  148.11332912002555 epsilon 0.01 steps 1000\n",
      "episode:  160 score:  135.30537968405648 epsilon 0.01 steps 1000\n",
      "episode:  161 score:  241.12189517687946 epsilon 0.01 steps 570\n",
      "episode:  162 score:  -198.42480083592937 epsilon 0.01 steps 327\n",
      "episode:  163 score:  191.63709785919755 epsilon 0.01 steps 624\n",
      "episode:  164 score:  103.75011241914387 epsilon 0.01 steps 1000\n",
      "episode:  165 score:  74.59651132445366 epsilon 0.01 steps 1000\n",
      "episode:  166 score:  107.79694695763028 epsilon 0.01 steps 1000\n",
      "episode:  167 score:  -29.63172216188417 epsilon 0.01 steps 189\n",
      "episode:  168 score:  72.36683443005734 epsilon 0.01 steps 1000\n",
      "episode:  169 score:  284.4697477297443 epsilon 0.01 steps 401\n",
      "episode:  170 score:  124.32366337450615 epsilon 0.01 steps 1000\n",
      "episode:  171 score:  207.0338756564704 epsilon 0.01 steps 465\n",
      "episode:  172 score:  142.68703664490863 epsilon 0.01 steps 1000\n",
      "episode:  173 score:  -42.96108580473774 epsilon 0.01 steps 383\n",
      "episode:  174 score:  92.69647932538683 epsilon 0.01 steps 1000\n",
      "episode:  175 score:  118.10368893490912 epsilon 0.01 steps 1000\n",
      "episode:  176 score:  116.8497291337518 epsilon 0.01 steps 1000\n",
      "episode:  177 score:  244.95108126452135 epsilon 0.01 steps 544\n",
      "episode:  178 score:  150.2572176946225 epsilon 0.01 steps 1000\n",
      "episode:  179 score:  205.16037110903656 epsilon 0.01 steps 525\n",
      "episode:  180 score:  197.7572972988437 epsilon 0.01 steps 384\n",
      "episode:  181 score:  154.08933984885473 epsilon 0.01 steps 1000\n",
      "episode:  182 score:  112.62054768579577 epsilon 0.01 steps 1000\n",
      "episode:  183 score:  24.84324700303253 epsilon 0.01 steps 307\n",
      "episode:  184 score:  -32.11905185738482 epsilon 0.01 steps 383\n",
      "episode:  185 score:  117.05058803985611 epsilon 0.01 steps 1000\n",
      "episode:  186 score:  -105.15373160766967 epsilon 0.01 steps 164\n",
      "episode:  187 score:  147.92325969171551 epsilon 0.01 steps 1000\n",
      "episode:  188 score:  213.4868904554292 epsilon 0.01 steps 705\n",
      "episode:  189 score:  120.54271613891201 epsilon 0.01 steps 804\n",
      "episode:  190 score:  104.5690234674725 epsilon 0.01 steps 1000\n",
      "episode:  191 score:  156.91599282966243 epsilon 0.01 steps 1000\n",
      "episode:  192 score:  217.73866470559466 epsilon 0.01 steps 382\n",
      "episode:  193 score:  -73.29065120240647 epsilon 0.01 steps 163\n",
      "episode:  194 score:  109.91310629891849 epsilon 0.01 steps 1000\n",
      "episode:  195 score:  59.593170295222954 epsilon 0.01 steps 1000\n",
      "episode:  196 score:  180.52747482651765 epsilon 0.01 steps 667\n",
      "episode:  197 score:  94.1300496975308 epsilon 0.01 steps 1000\n",
      "episode:  198 score:  188.26374675543377 epsilon 0.01 steps 842\n",
      "episode:  199 score:  245.54402195131996 epsilon 0.01 steps 393\n",
      "episode:  200 score:  122.91097423779368 epsilon 0.01 steps 1000\n",
      "episode:  201 score:  84.45253081304179 epsilon 0.01 steps 1000\n",
      "episode:  202 score:  164.79595860854866 epsilon 0.01 steps 405\n",
      "episode:  203 score:  131.0247548648338 epsilon 0.01 steps 1000\n",
      "episode:  204 score:  141.68287597134764 epsilon 0.01 steps 1000\n",
      "episode:  205 score:  133.6655991379894 epsilon 0.01 steps 1000\n",
      "episode:  206 score:  150.66304241518532 epsilon 0.01 steps 1000\n",
      "episode:  207 score:  107.39102623723332 epsilon 0.01 steps 1000\n",
      "episode:  208 score:  128.03156432190548 epsilon 0.01 steps 1000\n",
      "episode:  209 score:  135.35642074351182 epsilon 0.01 steps 1000\n",
      "episode:  210 score:  -131.8694443756553 epsilon 0.01 steps 126\n",
      "episode:  211 score:  -81.72239215322098 epsilon 0.01 steps 344\n",
      "episode:  212 score:  122.9277924758507 epsilon 0.01 steps 1000\n",
      "episode:  213 score:  -258.262606336884 epsilon 0.01 steps 105\n",
      "episode:  214 score:  225.50321379276107 epsilon 0.01 steps 581\n",
      "episode:  215 score:  114.79017997316362 epsilon 0.01 steps 1000\n",
      "episode:  216 score:  109.76198793640478 epsilon 0.01 steps 1000\n",
      "episode:  217 score:  111.11667113722297 epsilon 0.01 steps 1000\n",
      "episode:  218 score:  233.33976627429496 epsilon 0.01 steps 397\n",
      "episode:  219 score:  152.2879991151405 epsilon 0.01 steps 1000\n",
      "episode:  220 score:  259.7547906258387 epsilon 0.01 steps 291\n",
      "episode:  221 score:  91.98692125985322 epsilon 0.01 steps 1000\n",
      "episode:  222 score:  158.45790167173163 epsilon 0.01 steps 1000\n",
      "episode:  223 score:  173.12700920795194 epsilon 0.01 steps 1000\n",
      "episode:  224 score:  -57.271383306933444 epsilon 0.01 steps 158\n",
      "episode:  225 score:  252.75441032234224 epsilon 0.01 steps 407\n",
      "episode:  226 score:  262.7407180959867 epsilon 0.01 steps 756\n",
      "episode:  227 score:  164.72144618841358 epsilon 0.01 steps 1000\n",
      "episode:  228 score:  165.5780372464728 epsilon 0.01 steps 1000\n",
      "episode:  229 score:  -19.330236547942206 epsilon 0.01 steps 189\n",
      "episode:  230 score:  -181.05649509351946 epsilon 0.01 steps 293\n",
      "episode:  231 score:  234.33422357869685 epsilon 0.01 steps 462\n",
      "episode:  232 score:  84.14006448170437 epsilon 0.01 steps 1000\n",
      "episode:  233 score:  124.840359723186 epsilon 0.01 steps 1000\n",
      "episode:  234 score:  -129.42588594601395 epsilon 0.01 steps 179\n",
      "episode:  235 score:  129.31207376436492 epsilon 0.01 steps 1000\n",
      "episode:  236 score:  150.39696160820023 epsilon 0.01 steps 585\n",
      "episode:  237 score:  148.86143104127183 epsilon 0.01 steps 1000\n",
      "episode:  238 score:  169.78019167300786 epsilon 0.01 steps 1000\n",
      "episode:  239 score:  229.3761585825455 epsilon 0.01 steps 538\n",
      "episode:  240 score:  252.07030328651678 epsilon 0.01 steps 821\n",
      "episode:  241 score:  25.108258209438674 epsilon 0.01 steps 282\n",
      "episode:  242 score:  -147.0339328695556 epsilon 0.01 steps 144\n",
      "episode:  243 score:  228.90230153399952 epsilon 0.01 steps 445\n",
      "episode:  244 score:  14.730729108400695 epsilon 0.01 steps 411\n",
      "episode:  245 score:  160.83762232516764 epsilon 0.01 steps 1000\n",
      "episode:  246 score:  -129.2270239180275 epsilon 0.01 steps 372\n",
      "episode:  247 score:  114.28742403194066 epsilon 0.01 steps 1000\n",
      "episode:  248 score:  113.14673669850004 epsilon 0.01 steps 1000\n",
      "episode:  249 score:  222.84603433468175 epsilon 0.01 steps 420\n",
      "episode:  250 score:  130.79870817496627 epsilon 0.01 steps 1000\n",
      "episode:  251 score:  -65.03504985089165 epsilon 0.01 steps 161\n",
      "episode:  252 score:  139.39075448608335 epsilon 0.01 steps 1000\n",
      "episode:  253 score:  -94.95881801056625 epsilon 0.01 steps 213\n",
      "episode:  254 score:  -117.45401134556242 epsilon 0.01 steps 203\n",
      "episode:  255 score:  224.09379972004677 epsilon 0.01 steps 466\n",
      "episode:  256 score:  -103.41792459412592 epsilon 0.01 steps 228\n",
      "episode:  257 score:  117.64082905072215 epsilon 0.01 steps 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  258 score:  134.62217872854566 epsilon 0.01 steps 1000\n",
      "episode:  259 score:  153.00044894125972 epsilon 0.01 steps 1000\n",
      "episode:  260 score:  144.25147006925297 epsilon 0.01 steps 1000\n",
      "episode:  261 score:  -138.75963652585745 epsilon 0.01 steps 128\n",
      "episode:  262 score:  -143.36334148449416 epsilon 0.01 steps 110\n",
      "episode:  263 score:  -29.998919621267206 epsilon 0.01 steps 298\n",
      "episode:  264 score:  -163.93270174785658 epsilon 0.01 steps 754\n",
      "episode:  265 score:  206.0615789538214 epsilon 0.01 steps 393\n",
      "episode:  266 score:  251.2257149640614 epsilon 0.01 steps 434\n",
      "episode:  267 score:  126.25164983817527 epsilon 0.01 steps 1000\n",
      "episode:  268 score:  -32.26639803961764 epsilon 0.01 steps 181\n",
      "episode:  269 score:  -46.501452217020066 epsilon 0.01 steps 418\n",
      "episode:  270 score:  -25.478563563738426 epsilon 0.01 steps 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-518fd9a74a9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mterminal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mEPSILON_DECAY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c188fad74c00>\u001b[0m in \u001b[0;36magent_step\u001b[1;34m(self, state, reward, terminal)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_replay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Update the last state and last action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m### START CODE HERE (~2 Lines)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c188fad74c00>\u001b[0m in \u001b[0;36magent_train\u001b[1;34m(self, experiences)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mbatch_size1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mq_next_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mv_next_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_next_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterminals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m     \"\"\"\n\u001b[1;32m-> 1615\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1617\u001b[0m   def interleave(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   3956\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3957\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[1;32m-> 3958\u001b[1;33m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[0;32m   3959\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3960\u001b[0m       raise TypeError(\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    981\u001b[0m       \u001b[1;31m# TensorArrays and `None`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m       func_outputs = nest.map_structure(convert, func_outputs,\n\u001b[1;32m--> 983\u001b[1;33m                                         expand_composites=True)\n\u001b[0m\u001b[0;32m    984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m       \u001b[0mcheck_mutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_args_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    943\u001b[0m               (str(python_func), type(x)))\n\u001b[0;32m    944\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeps_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36mmark_as_return\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;31m# of a new identity operation that the stateful operations definitely don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;31m# depend on.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_returned_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3827\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3828\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m-> 3829\u001b[1;33m         \"Identity\", input=input, name=name)\n\u001b[0m\u001b[0;32m   3830\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3831\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1602\u001b[0m       \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_AddInputList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mop_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1604\u001b[1;33m       \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_AddInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_tf_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1606\u001b[0m   \u001b[1;31m# Add control inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(0, 501):\n",
    "    action=agent.agent_start()\n",
    "    terminal=0\n",
    "    while terminal!=1:\n",
    "        state,reward,terminal,info=env.step(action)\n",
    "        if terminal==True:\n",
    "            terminal=1\n",
    "        else:\n",
    "            terminal=0\n",
    "        action=agent.agent_step(state,reward,terminal)\n",
    "        if agent.epsilon > MIN_EPSILON:\n",
    "            agent.epsilon *= EPSILON_DECAY\n",
    "            agent.epsilon = max(MIN_EPSILON, agent.epsilon)\n",
    "    reward = agent.agent_message('get_sum_reward')\n",
    "    reward_episode.append(agent.sum_rewards)\n",
    "    no_episodes.append(episode)\n",
    "    episode_steps.append(agent.episode_steps)\n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    print('episode: ', episode,'score: ', agent.sum_rewards,\n",
    "            'epsilon %.2f' % agent.epsilon, 'steps', agent.episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(avg_reward_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tensorflow] *",
   "language": "python",
   "name": "conda-env-.conda-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
